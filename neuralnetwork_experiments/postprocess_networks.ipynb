{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Heavy-Tail Phenomenon in SGD\n",
    "\n",
    "This notebook contains the code for computing the tail indices for the neural networks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from models import alexnet, fc\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corollary 2.4 in Mohammadi 2014 - for 1d\n",
    "def alpha_estimator_one(m, X):\n",
    "    N = len(X)\n",
    "    n = int(N/m) # must be an integer\n",
    "    \n",
    "    X = X[0:n*m]\n",
    "    \n",
    "    Y = np.sum(X.reshape(n, m),1)\n",
    "    eps = np.spacing(1)\n",
    "\n",
    "    Y_log_norm =  np.log(np.abs(Y) + eps).mean()\n",
    "    X_log_norm =  np.log(np.abs(X) + eps).mean()\n",
    "    diff = (Y_log_norm - X_log_norm) / math.log(m)\n",
    "    return 1 / diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"PATH TO THE RESULTS FOLDER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained neural networks\n",
    "\n",
    "import glob\n",
    "\n",
    "dataset = 'mnist'\n",
    "# dataset = 'cifar10'\n",
    "\n",
    "loss = 'NLL'\n",
    "arch = 'fc'\n",
    "\n",
    "num_nets = 1000\n",
    "\n",
    "search_str = path + '*' + dataset + '_' + loss + '_' + arch + '*'\n",
    "\n",
    "results = {}\n",
    "hist_tr = {}\n",
    "hist_te = {}\n",
    "nets    = {}\n",
    "\n",
    "for folder_tmp in sorted(glob.glob(search_str)):\n",
    "    drive, path_and_file = os.path.splitdrive(folder_tmp)\n",
    "    dummy, folder = os.path.split(path_and_file)\n",
    "\n",
    "    base = path + folder\n",
    "    f = path + folder + '/training_history.hist'\n",
    "    if os.path.isfile(f):\n",
    "        print(folder)\n",
    "        results[folder] = torch.load(f)\n",
    "        hist_tr[folder] = torch.load(base + '/evaluation_history_TRAIN.hist',map_location='cpu')\n",
    "        hist_te[folder] = torch.load(base + '/evaluation_history_TEST.hist',map_location='cpu')\n",
    "        nets[folder]    = [] \n",
    "        for i in range(num_nets):\n",
    "            tmp_f = path + folder + '/net_' + str(i) + '.pyT'\n",
    "            nets[folder].append(torch.load(tmp_f, map_location='cpu'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the tail index for each step-size/batch-size pair\n",
    "\n",
    "depth = 3\n",
    "width = 128\n",
    "\n",
    "etas         = [0.0001, 0.001, 0.01, 0.015, 0.02, 0.025, 0.03, 0.04, 0.045, 0.05, 0.06, 0.070, 0.075, 0.08, 0.09, 0.1]\n",
    "batch_sizes  = [1, 5, 10]\n",
    "\n",
    "alphas_mc    = np.zeros((len(etas), len(batch_sizes), depth))-1\n",
    "\n",
    "for ei, eta in tqdm(enumerate(etas)):\n",
    "    for bi, bs in tqdm(enumerate(batch_sizes)):\n",
    "\n",
    "        print(ei,bi)\n",
    "        exp_name = '{}_{:04d}_{}_{}_{}_{:E}_{:04d}'.format(depth, width, dataset, loss, arch, eta, bs)\n",
    "        \n",
    "        if exp_name not in nets:\n",
    "            print(exp_name + \" does not exist\")\n",
    "            continue\n",
    "            \n",
    "        net = nets[exp_name]\n",
    "\n",
    "        weights = []\n",
    "        for i in range(depth):\n",
    "            weights.append([])\n",
    "\n",
    "        # record the layers in different arrays\n",
    "        for i in range(num_nets):\n",
    "            tmp_net = net[i]\n",
    "            for ix, p in enumerate(tmp_net.parameters()):\n",
    "                layer = p.detach().numpy()\n",
    "                layer = layer.reshape(-1,1)\n",
    "                weights[ix].append(layer)\n",
    "\n",
    "        for i in range(depth):\n",
    "            weights[i] = np.concatenate(weights[i], axis = 1)  \n",
    "\n",
    "        for i in range(depth):\n",
    "            tmp_weights = np.mean(weights[i], axis=1)\n",
    "            tmp_weights = tmp_weights.reshape(-1,1)\n",
    "            tmp_weights = tmp_weights - np.mean(tmp_weights)\n",
    "            tmp_alphas = [alpha_estimator_one(mm, tmp_weights) for mm in (2, 5, 10, 20, 50, 100, 500, 1000)]\n",
    "            alphas_mc[ei,bi,i] = np.median(tmp_alphas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "for i in range(depth):\n",
    "    plt.figure()\n",
    "    for ei, eta in enumerate(etas):\n",
    "        for bi, bs in enumerate(batch_sizes):\n",
    "            if(alphas_mc[ei,bi,i] > 0):\n",
    "                plt.plot(eta/bs, alphas_mc[ei,bi,i],'.')\n",
    "                plt.xlabel(\"eta/b\")\n",
    "                plt.ylabel(\"alpha\")\n",
    "    plt.title('Layer '+str(i+1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
